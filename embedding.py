# app.py
# -*- coding: utf-8 -*-
"""
å›½è©¦éå»å• Excelï¼ˆãƒ†ã‚­ã‚¹ãƒˆï¼‹ç”»åƒURLï¼‰ã‚’èª­ã¿è¾¼ã¿ã€
CLIPã§ãƒ†ã‚­ã‚¹ãƒˆï¼†ç”»åƒã‚’åŸ‹ã‚è¾¼ã¿ â†’ èåˆãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½œæˆã€‚
çµæœã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¤ã¤ã€t-SNEã§2Dãƒãƒƒãƒ”ãƒ³ã‚°ã—ã¦å¯è¦–åŒ–ã™ã‚‹ Streamlit ã‚¢ãƒ—ãƒªã€‚
"""

import io
import re
from typing import List, Optional

import numpy as np
import pandas as pd
import requests
from PIL import Image
from sklearn.manifold import TSNE

import streamlit as st
import torch
import clip  # openai/CLIP
import altair as alt


# ------------------------------------------------------------
# CLIP ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹ï¼‰
# ------------------------------------------------------------
@st.cache_resource
def load_clip_model():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model, preprocess = clip.load("ViT-B/32", device=device)
    return model, preprocess, device


def embed_text_clip(model, device, text: str) -> np.ndarray:
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’ CLIP ã§ 512æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ã«åŸ‹ã‚è¾¼ã‚€"""
    if not isinstance(text, str) or text.strip() == "":
        return np.zeros(512, dtype=np.float32)

    with torch.no_grad():
        tokens = clip.tokenize([text]).to(device)
        text_features = model.encode_text(tokens)
        text_features = text_features / text_features.norm(dim=-1, keepdim=True)
    return text_features.cpu().numpy()[0].astype(np.float32)


def drive_link_to_direct_url(link: str) -> Optional[str]:
    """Google Drive å…±æœ‰ãƒªãƒ³ã‚¯ã‚’ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰URL ã«å¤‰æ›"""
    if not isinstance(link, str):
        return None
    m = re.search(r"/d/([^/]+)/", link)
    if not m:
        return None
    file_id = m.group(1)
    return f"https://drive.google.com/uc?export=download&id={file_id}"


def load_image_from_drive(link: str) -> Optional[Image.Image]:
    """Google Drive ãƒªãƒ³ã‚¯ã‹ã‚‰ç”»åƒã‚’å–å¾—ã—ã¦ PIL ã«å¤‰æ›"""
    url = drive_link_to_direct_url(link)
    if url is None:
        return None
    try:
        resp = requests.get(url, timeout=15)
        resp.raise_for_status()
        img = Image.open(io.BytesIO(resp.content)).convert("RGB")
        return img
    except Exception:
        return None


def embed_image_clip(model, preprocess, device, link: str) -> np.ndarray:
    """ç”»åƒã‚’512æ¬¡å…ƒã®CLIPåŸ‹ã‚è¾¼ã¿ã«"""
    if not isinstance(link, str) or link.strip() == "":
        return np.zeros(512, dtype=np.float32)

    img = load_image_from_drive(link)
    if img is None:
        return np.zeros(512, dtype=np.float32)

    img_input = preprocess(img).unsqueeze(0).to(device)
    with torch.no_grad():
        image_features = model.encode_image(img_input)
        image_features = image_features / image_features.norm(dim=-1, keepdim=True)
    return image_features.cpu().numpy()[0].astype(np.float32)


def fuse_embeddings(text_vec: np.ndarray, image_vec: np.ndarray,
                    alpha: float = 0.7, beta: float = 0.3) -> np.ndarray:
    """ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’é‡ã¿ä»˜ãã§èåˆ"""
    v = alpha * text_vec + beta * image_vec
    norm = np.linalg.norm(v)
    if norm == 0:
        return v.astype(np.float32)
    return (v / norm).astype(np.float32)


# ============================================================
# Streamlit ã‚¢ãƒ—ãƒª UI
# ============================================================
st.set_page_config(page_title="å›½è©¦DB åŸ‹ã‚è¾¼ã¿ï¼‹t-SNEå¯è¦–åŒ–", layout="wide", page_icon="ğŸ¦·")

st.title("ğŸ¦· å›½è©¦éå»å•ï¼šãƒ†ã‚­ã‚¹ãƒˆï¼‹ç”»åƒ åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ & t-SNE å¯è¦–åŒ–ã‚¢ãƒ—ãƒª")

uploaded = st.file_uploader("å›½è©¦DB Excel (.xlsx) ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["xlsx"])

if uploaded is not None:
    df = pd.read_excel(uploaded)
    st.success(f"èª­ã¿è¾¼ã¿æˆåŠŸï¼š{df.shape[0]} è¡Œ Ã— {df.shape[1]} åˆ—")

    cols = list(df.columns)

    # IDåˆ—æ¨å®š
    default_id_col = None
    for cand in ["å•é¡ŒID", "id", "ID"]:
        if cand in cols:
            default_id_col = cand
            break

    # ç”»åƒåˆ—æ¨å®š
    default_img_col = None
    for cand in ["ç”»åƒURL", "img_url", "image_url"]:
        if cand in cols:
            default_img_col = cand
            break

    st.subheader("åˆ—ã®è¨­å®š")
    id_col = st.selectbox("å•é¡ŒID", options=cols,
                          index=cols.index(default_id_col) if default_id_col else 0)

    default_text_cols = [c for c in ["å•é¡Œæ–‡", "a", "b", "c", "d", "e", "è§£èª¬"] if c in cols]
    text_cols = st.multiselect("ãƒ†ã‚­ã‚¹ãƒˆã«ä½¿ã†åˆ—", options=cols,
                               default=default_text_cols if default_text_cols else [cols[0]])

    img_col = st.selectbox("ç”»åƒURLåˆ—ï¼ˆDriveãƒªãƒ³ã‚¯ï¼‰",
                           options=["ï¼ˆãªã—ï¼‰"] + cols,
                           index=(cols.index(default_img_col) + 1) if default_img_col else 0)

    use_image = img_col != "ï¼ˆãªã—ï¼‰"

    st.subheader("åŸ‹ã‚è¾¼ã¿ã®é‡ã¿")
    alpha = st.slider("ãƒ†ã‚­ã‚¹ãƒˆé‡ã¿ Î±", 0.0, 1.0, 0.7, 0.05)
    beta = 1.0 - alpha

    st.subheader("t-SNE è¨­å®š")
    perplexity = st.slider("perplexity", 5, 50, 30, 1)
    tsne_seed = st.number_input("random_state", value=42)

    color_col = st.selectbox("ãƒ—ãƒ­ãƒƒãƒˆã®è‰²åˆ†ã‘ï¼ˆä»»æ„ï¼‰",
                             options=["ï¼ˆãªã—ï¼‰"] + cols)

    run_button = st.button("åŸ‹ã‚è¾¼ã¿ï¼‹t-SNE å®Ÿè¡Œ", type="primary")

    if run_button:
        model, preprocess, device = load_clip_model()

        fused_vecs = []
        progress = st.progress(0)
        n = len(df)

        for i, row in df.iterrows():
            # ãƒ†ã‚­ã‚¹ãƒˆçµåˆ
            texts = []
            for c in text_cols:
                v = row.get(c, "")
                if isinstance(v, str):
                    texts.append(v.strip())
            full_text = "\n".join(texts)

            text_vec = embed_text_clip(model, device, full_text)

            if use_image:
                image_vec = embed_image_clip(model, preprocess, device, row.get(img_col, ""))
            else:
                image_vec = np.zeros(512, dtype=np.float32)

            vec = fuse_embeddings(text_vec, image_vec, alpha=alpha, beta=beta)
            fused_vecs.append(vec)

            if (i + 1) % 10 == 0:
                progress.progress((i + 1) / n)

        progress.progress(1.0)

        fused_arr = np.vstack(fused_vecs)
        emb_df = pd.DataFrame({"å•é¡ŒID": df[id_col]})
        for i in range(fused_arr.shape[1]):
            emb_df[f"emb_{i}"] = fused_arr[:, i]

        st.download_button(
            "â¬‡ åŸ‹ã‚è¾¼ã¿CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            emb_df.to_csv(index=False).encode("utf-8-sig"),
            file_name="kokushi_embedding_fused.csv",
            mime="text/csv"
        )

        st.subheader("t-SNE å®Ÿè¡Œä¸­â€¦")
        tsne = TSNE(
            n_components=2,
            perplexity=int(perplexity),
            random_state=int(tsne_seed),
            metric="cosine",
            init="random",
        )
        coords = tsne.fit_transform(fused_arr)

        vis_df = df.copy()
        vis_df["tsne_x"] = coords[:, 0]
        vis_df["tsne_y"] = coords[:, 1]

        chart = (
            alt.Chart(vis_df)
            .mark_circle(size=60, opacity=0.7)
            .encode(
                x="tsne_x:Q",
                y="tsne_y:Q",
                tooltip=[id_col] + text_cols[:2],
                color=(alt.Color(f"{color_col}:N") if color_col != "ï¼ˆãªã—ï¼‰" else alt.value("steelblue")),
            )
            .interactive()
        )
        st.altair_chart(chart, use_container_width=True)

        st.download_button(
            "â¬‡ t-SNEåº§æ¨™CSV",
            vis_df[[id_col, "tsne_x", "tsne_y"] + ([color_col] if color_col != "ï¼ˆãªã—ï¼‰" else [])]
            .to_csv(index=False).encode("utf-8-sig"),
            file_name="kokushi_tsne.csv",
            mime="text/csv"
        )


else:
    st.info("Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
